---
layout: article
title: 基于传统ML方法的CWRU故障诊断
mathjax: true
tags: CWRU
---

**Topic**: 用传统的机器学习方法对轴承进行故障诊断。

**Cooperators**: 友Q和友Min

* Task:
  
  * 数据处理
    - [x] 数据格式转化+载入
    - [x] 数据重叠采样
    - [x] 特征提取
    - [x] 样本生成+数据集划分
  
  * 模型训练
    - [x] KNN
    - [X] SVM
    - [x] XGboost
  
  * future

# 1. 数据处理

## 1.1 CWRU数据集

        CWRU轴承数据集是由美国凯斯西储大学（Case Western Reserve University，CRRU）获得的。该数据集获取的主要的实验设备包括：2 hp电动机，扭矩传感器和测力计。数据集的振动信号通过四种负载情况，以12 kHz的采样频率，在三个不同的地方设置加速度计收集。

        该数据集具有以下四种情况：正常状况，球缺陷（BD），外圈缺陷（OR）和内圈缺陷（IR）。每个故障条件具有三个度数：0.18、0.36和0.54 mm。九个有故障的轴承状况和正常状况共十种类别作为实验数据集，如下表所示：

<img title="" src="https://raw.githubusercontent.com/lzboo/ImgStg/main/2022/04/15-11-34-30-1.png" alt="1.png" width="437" data-align="center">

        原始数据以mat文件存储，每一个mat文件代表十类中的一类数据，同时一类数据同时具有三个加速度数据，即BA，DE，FE三列，将mat后缀文件转为csv文件后，通过pandas进行导入,得到十类数据。为了方便处理，转换为ndarray格式。

```python
1. cwru_0 = pd.read_csv('D:\\CWRU_0.csv',engine = 'python')    
2. cwru_1 = pd.read_csv('D:\\CWRU_1.csv',engine = 'python') 
...
```



## 1.2 数据重叠采样

          将数据从原始array数组中每隔1000个取样成为一个连续时域样本，采样重叠率为50%。

```python
# 本段代码较为冗余，后续将改为一个函数
1.	CWRU_0=[] 
2.	row_rand_array = np.arange(cwru_0.shape[0])
3.	j=0
4.	for i in range((int)(cwru_0.shape[0] / 500) - 1):
5.	    CWRU_0_ = cwru_0[row_rand_array[j:1000+j]]
6.	    CWRU_0.append(CWRU_0_)
7.	    j=j+500             
8.	print('------------')
9.	CWRU_0=np.array(CWRU_0)
10.	print(CWRU_0.shape)
```

## 1.3 特征提取

        由于原始数据是按12kHz的采样频率采样得到，因此实际上可以将数据看成大量采样点，再通过一系列采样点得到样本。本次实验，我们在该数据集上以1000个采样点为一个样本，提取数据时域和频域特征。

        我们构造了一个特征提取函数：feature(data,p1, p2)，特征提取时分别导入BA，DE, FE的数据分开提取。data.shape = (采样点总数,1)，p1表示开始点，p2表示结束点。最后，我们一共提取了12个特征（9个时域特征，3个频域特征）选取以下特征：

| 均值（mean）       | 方差（var）              | 均方根（rms）                     | 峭度（kurt）          |
|:--------------:|:--------------------:|:----------------------------:|:-----------------:|
| **偏度（skew）**   | **波形因子（boxing）**     | **波峰因子（peak）**               | **脉冲因子（impulse）** |
| **裕度因子（yudu）** | **包络谱最大幅值处频率（maxf）** | **一位序列信号幅值中位数处的概率密度估计（pdf）** | **一位序列香农信号熵**     |

### 1.3.1 时域特征

1. mean_(均值)：信号的平均。

2. var_(方差)：每个样本值与全体样本值的平均数之差的平方值的平均数，代表了信号能量的动态分量。

3.  rms_(均方根)：是信号的有效值。

4. kurt_(峭度因子)：表征概率密度分布曲线在平均值处峰值高低的特征数。

5. skew_(偏度因子)：是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。

6. form_ (波形因子)：波性因子是有效值（RMS）与整流平均值的比值。

7. par_(峰值因子)：是信号峰值与有效值（RMS）的比值，代表的是峰值在波形中的极端程度，**用来检测信号中有无冲击的指标。**

8. impulse_(脉冲因子)：是信号峰值与整流平均值的比值，**用来检测信号中有无冲击的指标。**

9. yudu_(裕度因子)：是信号峰值与方根幅值的比值。**用来检测机械设备的磨损状况。**

### 1.3.2 频域特征

10. 包络谱最大幅值处的频率（maxf）
    
    - 包络谱的求法是：目标信号→希尔伯特变换→得到解析信号→求解析信号的模→得到包络信号→傅里叶变换→得到Hilbert包络谱。
    
    - 希尔伯特变换:
      
      ```python
      # 求希尔伯特变换
      1.    T = 1/fs
      2.    N = len(data) 
      3.    analytic_signal = hilbert(data)
      
      # 求解析信号的模同时进行快速傅里叶变换:
      4.    am_enve = np.abs(analytic_signal).reshape(N,)
      5.    yf = fft(am_enve - np.mean(am_enve)) 
      
      # 得到包络谱后返回最大幅度的频率：
      6.    y_envsp = 2.0/N * np.abs(yf[0:N//2]).reshape(N//2,1)
      7.    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)
      8.    maxf = xf[np.argwhere(y_envsp=np.max(y_envsp))[0][0]] 
      ```

11. 一维序列信号幅值中位数处的概率密度估计（pdf）
    
    - 一维序列信号幅值中位数处的概率密度估计（pdf）
    
    - 定义一个hist_for_entropy()函数来计算信号幅值中位数处的概率密度估计。返回频次数组res,最大值、最小值，ncell。
      
      ```python
      1.    #对信号的直方图计算 为了计算信号幅值中位数处的概率密度估计   
      2.    def hist_for_entropy(s):
      3.            s = np.ravel(s)  #ravel转为一维数组
      4.            N = len(s)
      5.            s_max = np.max(s)
      6.            s_min = np.min(s)
      7.            delt = (s_max - s_min) / N
      8.            c_0 = s_min - delt / 2
      9.            c_N = s_max + delt / 2
      10.            ncell = int(np.ceil(np.sqrt(N)))  # ceil向上取整
      11.    
      12.            # c = f(s)
      13.            c = np.round((s - c_0) / (c_N - c_0) * ncell + 1/2) # 四舍五入取整
      14.    
      15.            # 计算分组数组出现的频次
      16.            res = np.zeros(ncell)
      17.            for i in range(0, N):
      18.                ind = int(c[i])
      19.                if ind >= 1 and ind <= ncell:
      20.                    res[ind-1] = res[ind-1] + 1
      21.    
      22.            return res, s_min, s_max, 
      ```
    
    - 计算一维序列信号幅值中位数处的概率密度估计：先有上述函数得到res, s_min, s_max, ncell，再归一化得到概率密度：
    
    - 计算出中位数处的概率密度估计：
      
      ```python
      23.    #一维序列信号幅值中位数处的概率密度估计
      24.    def pdf_for_median_am(s):
      25.            N = len(s)
      26.            res, s_min, s_max, ncell = hist_for_entropy(s)
      27.            # 归一化的到概率密度
      28.            pdf = res / N / (s_max - s_min) * ncell
      29.            
      30.            # 幅值中位数 映射 到直方图的组号
      31.            delt = (s_max - s_min) / N
      32.            c_min = s_min - delt / 2
      33.            c_max = s_max + delt / 2
      34.            
      35.            s_median = np.median(s)
      36.            s_median_icell = int(np.round((s_median - c_min) / (c_max - c_min) * ncell + 1/2))
      37.            return  pdf[s_median_icell]
      
      
      ```

12. 一维序列的香农熵（entroy）
    
    * 信息熵计算公式：
    
    * 变量的不确定性越大，熵也越大
    
    * 在一维序列中，数据值的范围并不是确定的，如果进行域值变换，使其转换到一个整数范围的话，就会丢失数据，因此，需要用Hist函数对x(n)的赋值范围进行分块:
      
      ```python
      1.	h,s_min, s_max, ncell = hist_for_entropy(s)  
      2.	# 无偏估计  
      3.	h = h[h!=0]  
      4.	N = np.sum(h)  
      ```
    
    * 信息熵计算：
      
      ```python
      1.	estimate = -np.sum(h*np.log(h)) / N  
      2.	sigma = np.sum(h*np.log2(h)**2)      
      3.	sigma = np.sqrt((sigma/N - estimate**2) / (N - 1))  
      4.	estimate = estimate + np.log(N) + np.log((s_max-s_min)/ncell)  
      5.	nbias = -(ncell-1)/(2*N)  
      6.	estimate = estimate - nbias  
      7.	return estimate  
      ```

### 1.3.3 特征提取函数 feature(data, p1, p2, fs)

```python
1.	''''' 特征提取 '''  
2.	fs = 12000  
3.	def feature(data, p1, p2,fs):  
4.	    sum = 0  
5.	    for i in range(p1, p2):  
6.	        sum += math.sqrt(abs(data[i]))  
7.	    #最大值  
8.	    max_ = data[p1:p2].max()  
9.	    #均值  
10.	    mean_ = data[p1:p2].mean()  
11.	    #绝对值  
12.	    abs_ = abs(data[p1:p2])  
13.	    #方差  
14.	    var_ = data[p1:p2].var()  
15.	    #标准差  
16.	    std_ = data[p1:p2].std()  
17.	    #均方根  
18.	    rms_ = math.sqrt(pow(mean_, 2) + pow(std_, 2))  
19.	      
20.	    xx=[x[0] for x in data]  
21.	    s = pd.Series(xx)  
22.	    # 峭度  
23.	    kurt_ = s.kurt()  
24.	    #print(kurt_EF)  
25.	    # 偏度  
26.	    skew_ = s.skew()    
27.	      
28.	    # 波形因子  
29.	    form_ = rms_ / (abs_.mean())  
30.	    # 峰值因子  
31.	    par_ = max_ / rms_  
32.	    # 脉冲因子  
33.	    impulse_ = max_ / (abs_.mean())  
34.	    # 裕度因子  
35.	    yudu_ = max_ / pow((sum / (p2 - p1)), 2)  
36.	    # 输入reshape成一维，用于后续频域分析  
37.	    data_s = data.ravel()  
38.	    # 包络谱最大幅值处的频率  
39.	    maxf_ = maxf_in_env_spectrum(data_s,fs)  
40.	    # 一维序列信号幅值中位数处的概率密度估计  
41.	    pdf_ = pdf_for_median_am(data_s)  
42.	    # 一维序列的香农信号熵  
43.	    entropy_ = shannom_entropy_for_hist(data_s)  
44.	      
45.	    feature_list = {"mean": mean_, "var": var_, "rms":rms_,   
46.	                    "kurt":kurt_, "skew":skew_, "form":form_,  
47.	                    "par":par_, "impulse":impulse_, "yudu":yudu_,  
48.	                    "maxf":maxf_, "entropy":entropy_, "pdf":pdf_}  
49.	    return feature_list  
```

### 1.3.4 特征提取结果及特征重要度

        该函数最终返回一个包含以上12个特征的feature_list：

```
{'mean': 0.004968823943943944,
'var': 0.01878744257741752,
'rms': 0.1371573249549707,
'kurt': -0.18708305756403298,
'skew': -0.03531797359099391,
'form': 1.2346087309932834,
'par': 2.9666661998080444,
'impulse': 3.6626719922256763,
'yudu': 4.2784956673448935,
'maxf': 156.312625250501,
'entropy': -0.5706768585895942,
'pdf': 2.3336084564390505}
```

        下图表反应了特征的重要程度，排序为：

     方差 > 均值 > 香农信号熵 > 波形因子 > 峭度因子 > 幅值中位数处的概率密度估计

<img title="" src="https://raw.githubusercontent.com/lzboo/ImgStg/main/2022/04/15-12-16-27-3.png" alt="3.png" width="477" data-align="center">

## 1.4 采样点转样本点

        时序数据重叠采样后，得到一系列三维采样矩阵，第一维表示最后的样本数量，第二维是每个样本的原始1000个采样点，第三维是三个加速度数据加上标签值。我们定义 data_to_samples(data_mkup_thousand)函数将采样点转为样本点。

        由于有三个不同的时序加速度数据，且BA加速度由于数据缺失暂不考虑，因此，需要从1000个采样点中分别得到DE和FE的时序数据：

```python
1.      number = 0  
2.	    for each_matrix in data_mkup_thousand:  
3.	        #label_of_sample = each_matrix[0][-1]  
4.	        # BA_of_each = each_matrix[:,0].reshape(-1,1)    
5.	        DE_of_each = each_matrix[:,1].reshape(-1,1)  
6.	        FE_of_each = each_matrix[:,2].reshape(-1,1)  
```

        此时DE_of_each及FE_of_each中是1000个采样点的数据，需要将他们作为特征提取函数的输入，最终得到12个特征的单行输出：

```python
1.	# 每个采样矩阵对DE特征提取  
2.	single_of_DE = []                        #p2个采样点提取
3.	dict_DE = feature(DE_of_each, p1, p2,fs)  
4.	for j in dict_DE.values():  
5.	    single_of_DE.append(j)  
6.	single_of_DE = np.array(single_of_DE).reshape(1,-1)   #转为array  
7.	      
8.	# 每个采样矩阵对FE特征提取  
9.	single_of_FE = []   #p2个采样点提取出的一个样本列表  
10.	dict_FE = feature(FE_of_each, p1, p2,fs)  
11.	for k in dict_FE.values():  
12.	    single_of_FE.append(k)  
13.	single_of_FE = np.array(single_of_FE).reshape(1,-1)  #转为array  
```

        同时，一个样本应同时具有DE和FE的十二个特征。通过concatenate可将两个一位向量整合成二维矩阵。由n个二维矩阵构成的三维矩阵，作为最终输出：

```python
1.	sample_matrix = np.concatenate((single_of_DE,single_of_FE),axis = 0).reshape(1,2,-1)   #样本矩阵       
2.	#sample_matrix = np.concatenate((single_of_BA,single_of_DE,single_of_FE),axis = 0).reshape(1,3,-1)  考虑BA情况  
3.	  
4.	if number == 0:  
5.	     matrix_to_samples = sample_matrix  
6.	else:  
7.	     matrix_to_samples = np.concatenate((matrix_to_samples,sample_matrix),axis = 0)  
8.	number = number + 1  
9.	return matrix_to_samples  

```

        十类数据分别输入：

```python
1.  matrix_to_samples_0 = data_to_samples(CWRU_0)
...
10. matrix_to_samples_9 = data_to_samples(CWRU_9)
```

## 1.5 数据集划分

### 1.5.1 train/val/test划分

        将特征提取后的每类样本集作为输入，按照7：2：1进行训练集，验证集和测试集划分。本实验采取生成索引列表并打乱索引列表，依次返回已打乱的索引列表中的不同区间的值，做为训练集、验证集和测试集对应的集合元素索引。再将索引值对应的样本作为输出：

```python
1.	def datasplit(data):  
2.	    total = data.shape[0]  
3.	    each = total//10  
4.	    indexlist = list(range(total))   #索引列表  
5.	    random.shuffle(indexlist)  
6.	    train_indice = indexlist[:7*each]  
7.	    eval_indice = indexlist[7*each:9*each]  
8.	    test_indice = indexlist[9*each:]  
9.	    return data[train_indice],data[eval_indice],data[test_indice]  
```

        将十类样本分别作为该函数的输出，分别十类得到输出，类似于分层抽样的思想：

```python
1.	cwru_0_train,cwru_0_eval,cwru_0_test = datasplit(matrix_to_samples_0)
...
10.	cwru_9_train,cwru_9_eval,cwru_9_test = datasplit(matrix_to_samples_9_) 
```

        最后，将不同类别的训练集整合成最终的训练集，测试集与验证集操作相同：

```python
1.	cwru_train = np.concatenate((cwru_0_train,cwru_1_train,cwru_2_train,cwru_3_train,cwru_4_train,cwru_5_train,cwru_6_train,cwru_7_train,cwru_8_train,cwru_9_train),axis = 0)  
2.	cwru_eval = np.concatenate((cwru_0_eval,cwru_1_eval,cwru_2_eval,cwru_3_eval,cwru_4_eval,cwru_5_eval,cwru_6_eval,cwru_7_eval,cwru_8_eval,cwru_9_eval),axis = 0)  
3.	cwru_test = np.concatenate((cwru_0_test,cwru_1_test,cwru_2_test,cwru_3_test,cwru_4_test,cwru_5_test,cwru_6_test,cwru_7_test,cwru_8_test,cwru_9_test),axis = 0) 
```

### 1.5.2 随机打乱

        定义函数suffle(data,label)，对样本设置索引，打乱索引值，从而实现打乱样本的特征标签对。 

```python
1.	''''' 打乱数据 '''  
2.	  
3.	def shuffle(data,label):      
4.	    index = [i for i in range(len(data))]   
5.	    random.shuffle(index)   
6.	    data = data[index]  
7.	    label = label[index]   
8.	    return data,label  
9.	  
10.	X_train_DE, y_train_DE =  shuffle(cwru_train[:,0,0:12], cwru_train[:,0,12])  
11.	X_eval_DE, y_eval_DE =  shuffle(cwru_eval[:,0,0:12], cwru_eval[:,0,12])  
12.	X_test_DE, y_test_DE =  shuffle(cwru_test[:,0,0:12], cwru_test[:,0,12])  
13.	  
14.	X_train_FE, y_train_FE =  shuffle(cwru_train[:,1,0:12], cwru_train[:,1,12])  
15.	X_eval_FE, y_eval_FE =  shuffle(cwru_eval[:,1,0:12], cwru_eval[:,1,12])  
16.	X_test_FE, y_test_FE =  shuffle(cwru_test[:,1,0:12], cwru_test[:,1,12])  

```

        

# 2. 模型训练

## 2.1 KNN（测试集样本数目：338）

        调用sk-learn中的KNeighborsClassifier方法对数据进行分类。对不同k值的预测正确率可视化，并选取k的最优（k=5）值绘制混淆矩阵。

### 2.1.1 代码

```python
1.	''''' knn训练 '''  
2.	training_accuracy = []  
3.	test_accuracy = []  
4.	neighbors_settings = range(1,45)  
5.	  
6.	for n_neighbors in neighbors_settings:  
7.	    knn = KNeighborsClassifier(n_neighbors)    #实例化KNN模型  
8.	    knn.fit(X_train_DE, y_train_DE)      #放入训练数据进行训练  
9.	    # print(knn.predict(cwru_eval[:,:-1]))           #打印预测内容  
10.	    # print(cwru_test[:3])     #实际标签  
11.	    training_accuracy.append(knn.score(X_train_DE, y_train_DE))  
12.	    #print(knn.score(cwru_train[:,0,0:12],cwru_train[:,0,12]))  
13.	    test_accuracy.append(knn.score(X_test_DE, y_test_DE))  
14.	    #print("k={}: accuracy: {:.3f}% ".format(n_neighbors,knn.score(X_test_DE, y_test_DE)*100))  
15.	#max_index = eval_accuracy.index(max(eval_accuracy))  
16.	#print(max_index, max(eval_accuracy))  
17.	      
18.	plt.plot(neighbors_settings, training_accuracy, label="training accuracy")  
19.	plt.plot(neighbors_settings, test_accuracy, label="test accuracy")  
20.	plt.xlabel("n_neighbors")  
21.	plt.ylabel("Accuracy")  
22.	plt.legend()  
23.	  
24.	#X_test_DE.shape  
```

### 2.1.2 结果分析

<img title="" src="https://raw.githubusercontent.com/lzboo/ImgStg/main/2022/04/15-12-31-51-4.png" alt="4.png" data-align="center" width="337">

        上图反应了不同k值下测试集的准确率，可以发现曲线近似随着k值的增加单调递减。我们分析出现这种问题的原因是：特征的量级比较小，不同特征之间的相似度较大，区分度不明显。可以通过对特征进行归一化处理改进这一问题。

<img src="https://raw.githubusercontent.com/lzboo/ImgStg/main/2022/04/15-12-34-00-5.png" title="" alt="5.png" data-align="center">

        由混淆矩阵可以得出：测试集中的338个样本在k=4的模型分类下，26个样本分类错误，分类准确率达92.3%。

## 2.2 SVM
### 2.2.1 代码
```python
lin_svc = svm.SVC(kernel='linear').fit(X_train_DE, y_train_DE)
rbf_svc = svm.SVC(kernel='rbf').fit(X_train_DE, y_train_DE)
poly_svc = svm.SVC(kernel='poly').fit(X_train_DE, y_train_DE)

title = ['linear kernel',
          'RBF kernel',
          'polynomial kernel']

for i, model in enumerate((lin_svc, rbf_svc, poly_svc)):
    print(title[i])
    model = model.predict(X_test_DE)
    print(classification_report(y_test_DE, model))
    print("------------------------------")
```

```
linear kernel
              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00       128
         1.0       1.00      0.96      0.98        25
         2.0       0.77      0.85      0.81        27
         3.0       0.81      0.50      0.62        26
         4.0       0.96      1.00      0.98        26
         6.0       1.00      1.00      1.00        26
         8.0       0.89      1.00      0.94        54
         9.0       1.00      1.00      1.00        26

    accuracy                           0.95       338
   macro avg       0.93      0.91      0.92       338
weighted avg       0.95      0.95      0.94       338

------------------------------
RBF kernel
              precision    recall  f1-score   support

         0.0       0.60      0.99      0.75       128
         1.0       0.00      0.00      0.00        25
         2.0       0.00      0.00      0.00        27
         3.0       0.00      0.00      0.00        26
         4.0       0.34      1.00      0.50        26
         6.0       0.00      0.00      0.00        26
         8.0       0.53      0.31      0.40        54
         9.0       1.00      0.65      0.79        26

    accuracy                           0.55       338
   macro avg       0.31      0.37      0.30       338
weighted avg       0.41      0.55      0.45       338

------------------------------
polynomial kernel
              precision    recall  f1-score   support

         0.0       0.38      1.00      0.55       128
         1.0       0.00      0.00      0.00        25
         2.0       0.00      0.00      0.00        27
         3.0       0.00      0.00      0.00        26
         4.0       0.00      0.00      0.00        26
         6.0       0.00      0.00      0.00        26
         8.0       0.00      0.00      0.00        54
         9.0       0.00      0.00      0.00        26

    accuracy                           0.38       338
   macro avg       0.05      0.12      0.07       338
weighted avg       0.14      0.38      0.21       338

```


## 2.3 XGboost

        CWRU数据集分类在XGboost模型上可以取得很好的效果，该模型是大规模并行boosted tree的工具，主要参数及含义如下：

### 2.3.1 代码

```python
1.	''''' XGboost训练 '''  
2.	# 算法参数  
3.	params = {  
4.	    'booster': 'gbtree',             # 基础模型：'决策树 '  
5.	    'objective': 'multi:softmax',    # 目标： '多分类：softmax'  
6.	    'num_class': 10,                 # 类别个数： ' 10 '      
7.	    'gamma': 0.1,                    # min_split_loss  
8.	    'max_depth': 6,                  # 树深 ： 该值越大，模型越复杂  
9.	    'lambda': 2,                     # L2正则化权重  
10.	    'subsample': 0.7,                # 构建每棵树对样本采样率  
11.	    'colsample_bytree': 0.75,        # 特征采样率  
12.	    'min_child_weight': 3,           # 节点分裂阈值  
13.	    'eta': 0.1,                      # 学习率  
14.	}  
15.	  
16.	dtrain = xgb.DMatrix(X_train_DE, y_train_DE)  
17.	  
18.	model = xgb.XGBClassifier(**params)  
19.	model.fit(X_train_DE, y_train_DE)  
20.	pred_xgboost = model.predict(X_test_DE)  
21.	print(classification_report(y_test_DE, pred_xgboost))  
```

### 2.3.2 结果分析
```
           precision    recall  f1-score   support

         0.0       1.00      1.00      1.00       128
         1.0       1.00      0.96      0.98        25
         2.0       0.79      1.00      0.89        27
         3.0       1.00      0.73      0.84        26
         4.0       1.00      1.00      1.00        26
         6.0       1.00      1.00      1.00        26
         8.0       0.98      1.00      0.99        54
         9.0       1.00      1.00      1.00        26

    accuracy                           0.98       338
   macro avg       0.97      0.96      0.96       338
weighted avg       0.98      0.98      0.98       338
```

        上表反应了使用XGboost模型进行分类时，不同类别分类的准确率，召回率，f1评价值。可以看出，XGboost分类器表现优异，各项指标均逼近100%。

        

<img title="" src="https://raw.githubusercontent.com/lzboo/ImgStg/main/2022/04/15-12-40-47-7.png" alt="7.png" width="329" data-align="center">

        由混淆矩阵可以得出：测试集中的338个样本XGboost模型分类下，仅有6个样本分类错误，分类准确率达98.2%。



# 3. 总结与心得

        这个项目主要分为：**原始数据处理—>训练模型—>测试模型**三大部分。其中，原始数据处理需要耗费大量的时间，现实中收集到的数据很多是无法直接使用的，数据清洗、数据转换、样本特征的选择与提取等至关重要。

1.  **数据处理**：

        CWRU原始数据集是通过时域采样得到的，无法直接送入传统的机器学习模型，需要预处理：

* 以1000个采样点作为一个样本数据，以50%的重叠率生成样本，这样可以避免测试样本的随机性造成的影响；

* 从时域和频域两个方面提取了12个特征，但是重要程度最高的两个特征却是最为简单的均值和方差！这个现象给了我们很大的启发：在求解问题的时候，一味的追求复杂高深并不一定可以达到好的效果，不能忽略那些看似简单基础的因素。

        之前看到一种描述：“数据的质量与算力决定了ML的上限，而算法则是去逼近这个

上限”。

2. **模型训练**：
   
   我们选取了3个机器学习领域最基础的模型：KNN,SVM，XGboost。事实证明：XGboost真是上分大杀器！！！

3. **心得**：
   
   现在大家的研究注意力可能大多集中在深度学习方面，焦点总放在网络的搭建上，不可否认，深度学习的兴起以算力为代价解放了人力（专家、特征选择提取等），是很划算的，因为科学的目的就是为了“解放生产力”嘛。但是传统的机器学习仍有很大探索发展空间，我也是在提醒自己，DL只是AI领域很小的一部分，不要因为“fellow the fashion”而忽视了其他待开采的土地！！（很重要）

# 4 future

* 后面计划“追赶一下潮流”，用DL的一些方法再研究研究故障诊断，因为还有很多精彩的idea等着我！！！
